
@misc{Colas.Karch.ea_2022,
  title = {Vygotskian {{Autotelic Artificial Intelligence}}: {{Language}} and {{Culture Internalization}} for {{Human-Like AI}}},
  shorttitle = {Vygotskian {{Autotelic Artificial Intelligence}}},
  author = {Colas, C{\'e}dric and Karch, Tristan and {Moulin-Frier}, Cl{\'e}ment and Oudeyer, Pierre-Yves},
  year = {2022},
  month = jun,
  number = {arXiv:2206.01134},
  eprint = {2206.01134},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Building autonomous artificial agents able to grow open-ended repertoires of skills across their lives is one of the fundamental goals of AI. To that end, a promising developmental approach recommends the design of intrinsically motivated agents that learn new skills by generating and pursuing their own goals \textemdash{} autotelic agents. However, despite recent progress, existing algorithms still show serious limitations in terms of goal diversity, exploration, generalization or skill composition. This perspective calls for the immersion of autotelic agents into rich socio-cultural worlds, an immensely important attribute of our environment that is mostly omitted in modern AI, including deep reinforcement learning research. We focus on language especially, and how its structure and content may support the development of new cognitive functions in artificial agents, just like it does in humans. Indeed, most of our skills could not be learned in isolation. Formal education teaches us to reason systematically, books teach us history, and YouTube might teach us how to cook. Most importantly, our values, traditions, norms and most of our goals are cultural in essence. This knowledge, and some argue, some of our highest cognitive functions such as abstraction, compositional imagination or relational thinking, are formed through linguistic and cultural interactions with others. Inspired by the seminal work of the developmentalist Vygotsky, we suggest the design of Vygotskian autotelic agents able to interact with others and, more importantly, able to internalize these interactions within the agent so as to transform them into cognitive tools supporting the development of new cognitive functions. This perspective paper finds its inspiration in the work of psychologists and philosophers to propose a new AI paradigm in the quest for artificial lifelong skill discovery. It justifies the approach by uncovering several examples of new artificial cognitive functions emerging from interactions between language and embodiment in recent works at the intersection of deep reinforcement learning and natural language processing. Looking forward, it highlights future opportunities and challenges for Vygotskian Autotelic AI research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {【学科】：AGI,【学科】：AI,【感觉】：有趣,【感觉】：重要,【进度】：准备进行,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Colas_Karch_et-al_2022_Vygotskian Autotelic Artificial Intelligence - Language and Culture.docx;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Colas_Karch_et-al_2022_Vygotskian Autotelic Artificial Intelligence - Language and Culture.md;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Colas_Karch_et-al_2022_Vygotskian Autotelic Artificial Intelligence - Language and Culture.pdf}
}

@misc{Liang_2022,
  title = {Brainish: {{Formalizing A Multimodal Language}} for {{Intelligence}} and {{Consciousness}}},
  shorttitle = {Brainish},
  author = {Liang, Paul Pu},
  year = {2022},
  month = may,
  number = {arXiv:2205.00001},
  eprint = {2205.00001},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Having a rich multimodal inner language is an important component of human intelligence that enables several necessary core cognitive functions such as multimodal prediction, translation, and generation. Building upon the Conscious Turing Machine (CTM), a machine model for consciousness proposed by Blum and Blum [13], we describe the desiderata of a multimodal language called BRAINISH, comprising words, images, audio, and sensations combined in representations that the CTM's processors use to communicate with each other. We define the syntax and semantics of BRAINISH before operationalizing this language through the lens of multimodal artificial intelligence, a vibrant research area studying the computational tools necessary for processing and relating information from heterogeneous signals. Our general framework for learning BRAINISH involves designing (1) unimodal encoders to segment and represent unimodal data, (2) a coordinated representation space that relates and composes unimodal features to derive holistic meaning across multimodal inputs, and (3) decoders to map multimodal representations into predictions (for fusion) or raw data (for translation or generation). Through discussing how BRAINISH is crucial for communication and coordination in order to achieve consciousness in the CTM, and by implementing a simple version of BRAINISH and evaluating its capability of demonstrating intelligence on multimodal prediction and retrieval tasks on several real-world image, text, and audio datasets, we argue that such an inner language will be important for advances in machine models of intelligence and consciousness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {【学科】：AGI,【学科】：AI,【感觉】：有趣,【感觉】：重要,【进度】：准备进行,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Liang_2022_Brainish - Formalizing A Multimodal Language for Intelligence and Consciousness.docx;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Liang_2022_Brainish - Formalizing A Multimodal Language for Intelligence and Consciousness.md;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Liang_2022_Brainish - Formalizing A Multimodal Language for Intelligence and Consciousness.pdf}
}

@misc{Reser_2022,
  title = {A {{Computational Architecture}} for {{Machine Consciousness}} and {{Artificial Superintelligence}}: {{Updating Working Memory Iteratively}}},
  author = {Reser, Jared Edward},
  year = {2022},
  month = may,
  abstract = {This theoretical article examines how to construct human-like working memory and thought processes within a computer. There should be two working memory stores, one analogous to sustained firing in association cortex, and one analogous to synaptic potentiation in the cerebral cortex. These stores must be constantly updated with new representations that arise from either environmental stimulation or internal processing. They should be updated continuously, and in an iterative fashion, meaning that, in the next state, some items in the set of coactive items should always be retained. Thus, the set of concepts coactive in working memory will evolve gradually and incrementally over time. This makes each state is a revised iteration of the preceding state and causes successive states to overlap and blend with respect to the set of representations they contain. As new representations are added and old ones are subtracted, some remain active for several seconds over the course of these changes. This persistent activity, similar to that used in artificial recurrent neural networks, is used to spread activation energy throughout the global workspace to search for the next associative update. The result is a chain of associatively linked intermediate states that are capable of advancing toward a solution or goal. Iterative updating is conceptualized here as an information processing strategy, a computational and neurophysiological determinant of the stream of thought, and an algorithm for designing and programming artificial intelligence.},
  keywords = {【学科】：AI,【感觉】：有趣,【感觉】：重要,【进度】：准备进行},
  file = {/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Reser_2022_A Computational Architecture for Machine Consciousness and Artificial.docx;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Reser_2022_A Computational Architecture for Machine Consciousness and Artificial.md;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Reser_2022_A Computational Architecture for Machine Consciousness and Artificial.pdf}
}


