
@misc{Colas.Karch.ea_2022_VygotskianAutotelicArtificialIntelligenceLanguageCultureInternalizationHumanLikeAI,
  title = {Vygotskian {{Autotelic Artificial Intelligence}}: {{Language}} and {{Culture Internalization}} for {{Human-Like AI}}},
  shorttitle = {Vygotskian {{Autotelic Artificial Intelligence}}},
  author = {Colas, C{\'e}dric and Karch, Tristan and {Moulin-Frier}, Cl{\'e}ment and Oudeyer, Pierre-Yves},
  year = {2022},
  month = jun,
  number = {arXiv:2206.01134},
  eprint = {2206.01134},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Building autonomous artificial agents able to grow open-ended repertoires of skills across their lives is one of the fundamental goals of AI. To that end, a promising developmental approach recommends the design of intrinsically motivated agents that learn new skills by generating and pursuing their own goals \textemdash{} autotelic agents. However, despite recent progress, existing algorithms still show serious limitations in terms of goal diversity, exploration, generalization or skill composition. This perspective calls for the immersion of autotelic agents into rich socio-cultural worlds, an immensely important attribute of our environment that is mostly omitted in modern AI, including deep reinforcement learning research. We focus on language especially, and how its structure and content may support the development of new cognitive functions in artificial agents, just like it does in humans. Indeed, most of our skills could not be learned in isolation. Formal education teaches us to reason systematically, books teach us history, and YouTube might teach us how to cook. Most importantly, our values, traditions, norms and most of our goals are cultural in essence. This knowledge, and some argue, some of our highest cognitive functions such as abstraction, compositional imagination or relational thinking, are formed through linguistic and cultural interactions with others. Inspired by the seminal work of the developmentalist Vygotsky, we suggest the design of Vygotskian autotelic agents able to interact with others and, more importantly, able to internalize these interactions within the agent so as to transform them into cognitive tools supporting the development of new cognitive functions. This perspective paper finds its inspiration in the work of psychologists and philosophers to propose a new AI paradigm in the quest for artificial lifelong skill discovery. It justifies the approach by uncovering several examples of new artificial cognitive functions emerging from interactions between language and embodiment in recent works at the intersection of deep reinforcement learning and natural language processing. Looking forward, it highlights future opportunities and challenges for Vygotskian Autotelic AI research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {【学科】：AGI,【学科】：AI,【感觉】：有趣,【感觉】：重要,【进度】：准备进行,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Colas_Karch_et-al_2022_Vygotskian Autotelic Artificial Intelligence - Language and Culture.docx;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Colas_Karch_et-al_2022_Vygotskian Autotelic Artificial Intelligence - Language and Culture.md;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Colas_Karch_et-al_2022_Vygotskian Autotelic Artificial Intelligence - Language and Culture.pdf}
}

@misc{Graves.Wayne.ea_2014_NeuralTuringMachines,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  month = dec,
  number = {arXiv:1410.5401},
  eprint = {1410.5401},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {【内容】：图灵机,【内容】：神经网络,【内容】：神经网络图灵机,【学科】：AI,【学科】：计算机科学,【感觉】：有趣,【进度】：准备进行,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Graves_Wayne_et-al_2014_Neural Turing Machines.md;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Graves_Wayne_et-al_2014_Neural Turing Machines.pdf;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Graves_Wayne_et-al_2014_Neural Turing Machines.zip}
}

@misc{Hudson.Manning_2019_LearningAbstractionNeuralStateMachine,
  title = {Learning by {{Abstraction}}: {{The Neural State Machine}}},
  shorttitle = {Learning by {{Abstraction}}},
  author = {Hudson, Drew A. and Manning, Christopher D.},
  year = {2019},
  month = nov,
  number = {arXiv:1907.03950},
  eprint = {1907.03950},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We introduce the Neural State Machine, seeking to bridge the gap between the neural and symbolic views of AI and integrate their complementary strengths for the task of visual reasoning. Given an image, we first predict a probabilistic graph that represents its underlying semantics and serves as a structured world model. Then, we perform sequential reasoning over the graph, iteratively traversing its nodes to answer a given question or draw a new inference. In contrast to most neural architectures that are designed to closely interact with the raw sensory data, our model operates instead in an abstract latent space, by transforming both the visual and linguistic modalities into semantic concept-based representations, thereby achieving enhanced transparency and modularity. We evaluate our model on VQA-CP and GQA, two recent VQA datasets that involve compositionality, multi-step inference and diverse reasoning skills, achieving state-of-the-art results in both cases. We provide further experiments that illustrate the model's strong generalization capacity across multiple dimensions, including novel compositions of concepts, changes in the answer distribution, and unseen linguistic structures, demonstrating the qualities and efficacy of our approach.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {【内容】：神经网络,【内容】：神经网络状态机,【内容】：计算机视觉,【学科】：AI,【学科】：计算机科学,【感觉】：有趣,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Hudson_Manning_2019_Learning by Abstraction - The Neural State Machine.md;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Hudson_Manning_2019_Learning by Abstraction - The Neural State Machine.pdf}
}

@article{Letelier.Marin.ea_2003_AutopoieticMRsystems,
  title = {Autopoietic and ({{M}},{{R}}) Systems},
  author = {Letelier, Juan Carlos and Mar{\'{\i}}n, Gonzalo and Mpodozis, Jorge},
  year = {2003},
  month = may,
  journal = {Journal of Theoretical Biology},
  volume = {222},
  number = {2},
  pages = {261--272},
  issn = {00225193},
  doi = {10.1016/S0022-5193(03)00034-1},
  abstract = {From the many attempts to produce a conceptual framework for the organization of living systems, the notions of (M,R) systems and Autopoiesis stand out for their rigor, their presupposition of the circularity of metabolism, and the new epistemologies that they imply. From their inceptions, these two notions have been essentially disconnected because each has defined its own language and tools. Here we demonstrate the existence of a deep conceptual link between (M,R) systems and Autopoietic systems. This relationship permits us to posit that Autopoietic systems, which have been advanced as capturing the central aspects of living systems, are a subset of (M,R) systems. This result, in conjunction with previous theorems proved by Rosen, can be used to outline a demonstration that the operation of Autopoietic systems cannot be simulated by Turing machines. This powerful result shows the potential of linking these two models. Finally, we suggest that the formalism of (M,R) systems could be used to model the circularity of metabolism.},
  langid = {english},
  keywords = {【内容】：因果涌现,【学科】：复杂系统,【项目】：因果涌现读书会},
  file = {/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Letelier_Marı́n_et-al_2003_Autopoietic and (M,R) systems.md;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Letelier_Marı́n_et-al_2003_Autopoietic and (M,R) systems.pdf}
}

@misc{Liang_2022_BrainishFormalizingMultimodalLanguageIntelligenceConsciousness,
  title = {Brainish: {{Formalizing A Multimodal Language}} for {{Intelligence}} and {{Consciousness}}},
  shorttitle = {Brainish},
  author = {Liang, Paul Pu},
  year = {2022},
  month = may,
  number = {arXiv:2205.00001},
  eprint = {2205.00001},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Having a rich multimodal inner language is an important component of human intelligence that enables several necessary core cognitive functions such as multimodal prediction, translation, and generation. Building upon the Conscious Turing Machine (CTM), a machine model for consciousness proposed by Blum and Blum [13], we describe the desiderata of a multimodal language called BRAINISH, comprising words, images, audio, and sensations combined in representations that the CTM's processors use to communicate with each other. We define the syntax and semantics of BRAINISH before operationalizing this language through the lens of multimodal artificial intelligence, a vibrant research area studying the computational tools necessary for processing and relating information from heterogeneous signals. Our general framework for learning BRAINISH involves designing (1) unimodal encoders to segment and represent unimodal data, (2) a coordinated representation space that relates and composes unimodal features to derive holistic meaning across multimodal inputs, and (3) decoders to map multimodal representations into predictions (for fusion) or raw data (for translation or generation). Through discussing how BRAINISH is crucial for communication and coordination in order to achieve consciousness in the CTM, and by implementing a simple version of BRAINISH and evaluating its capability of demonstrating intelligence on multimodal prediction and retrieval tasks on several real-world image, text, and audio datasets, we argue that such an inner language will be important for advances in machine models of intelligence and consciousness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {【学科】：AGI,【学科】：AI,【感觉】：有趣,【感觉】：重要,【进度】：准备进行,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Liang_2022_Brainish - Formalizing A Multimodal Language for Intelligence and Consciousness.docx;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Liang_2022_Brainish - Formalizing A Multimodal Language for Intelligence and Consciousness.md;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Liang_2022_Brainish - Formalizing A Multimodal Language for Intelligence and Consciousness.pdf}
}

@article{Mayner.Marshall.ea_2018_PyPhitoolboxintegratedinformationtheory,
  ids = {Mayner.Marshall.ea_2018},
  title = {{{PyPhi}}: {{A}} Toolbox for Integrated Information Theory},
  shorttitle = {{{PyPhi}}},
  author = {Mayner, William G. P. and Marshall, William and Albantakis, Larissa and Findlay, Graham and Marchman, Robert and Tononi, Giulio},
  editor = {Blackwell, Kim T.},
  year = {2018},
  month = jul,
  journal = {PLOS Computational Biology},
  volume = {14},
  number = {7},
  pages = {e1006343},
  issn = {1553-7358},
  doi = {10/gdxwmx},
  abstract = {Integrated information theory provides a mathematical framework to fully characterize the cause-effect structure of a physical system. Here, we introduce PyPhi, a Python software package that implements this framework for causal analysis and unfolds the full cause-effect structure of discrete dynamical systems of binary elements. The software allows users to easily study these structures, serves as an up-to-date reference implementation of the formalisms of integrated information theory, and has been applied in research on complexity, emergence, and certain biological questions. We first provide an overview of the main algorithm and demonstrate PyPhi's functionality in the course of analyzing an example system, and then describe details of the algorithm's design and implementation. PyPhi can be installed with Python's package manager via the command `pip install pyphi' on Linux and macOS systems equipped with Python 3.4 or higher. PyPhi is open-source and licensed under the GPLv3; the source code is hosted on GitHub at https://github.com/ wmayner/pyphi. Comprehensive and continually-updated documentation is available at https://pyphi.readthedocs.io. The pyphi-users mailing list can be joined at https://groups. google.com/forum/\#!forum/pyphi-users. A web-based graphical interface to the software is available at http://integratedinformationtheory.org/calculate.html.},
  langid = {english},
  keywords = {【内容】：工具包,【内容】：整合信息论,【学科】：信息论,【学科】：认知科学,【感觉】：有趣,【感觉】：重要,【进度】：准备进行},
  file = {/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Mayner_Marshall_et-al_2018_PyPhi - A toolbox for integrated information theory.pdf}
}

@article{Oizumi.Albantakis.ea_2014_PhenomenologyMechanismsConsciousnessIntegratedInformationTheory30,
  title = {From the {{Phenomenology}} to the {{Mechanisms}} of {{Consciousness}}: {{Integrated Information Theory}} 3.0},
  shorttitle = {From the {{Phenomenology}} to the {{Mechanisms}} of {{Consciousness}}},
  author = {Oizumi, Masafumi and Albantakis, Larissa and Tononi, Giulio},
  editor = {Sporns, Olaf},
  year = {2014},
  month = may,
  journal = {PLoS Computational Biology},
  volume = {10},
  number = {5},
  pages = {e1003588},
  issn = {1553-7358},
  doi = {10/sqz},
  abstract = {This paper presents Integrated Information Theory (IIT) of consciousness 3.0, which incorporates several advances over previous formulations. IIT starts from phenomenological axioms: information says that each experience is specific \textendash{} it is what it is by how it differs from alternative experiences; integration says that it is unified \textendash{} irreducible to noninterdependent components; exclusion says that it has unique borders and a particular spatio-temporal grain. These axioms are formalized into postulates that prescribe how physical mechanisms, such as neurons or logic gates, must be configured to generate experience (phenomenology). The postulates are used to define intrinsic information as ``differences that make a difference'' within a system, and integrated information as information specified by a whole that cannot be reduced to that specified by its parts. By applying the postulates both at the level of individual mechanisms and at the level of systems of mechanisms, IIT arrives at an identity: an experience is a maximally irreducible conceptual structure (MICS, a constellation of concepts in qualia space), and the set of elements that generates it constitutes a complex. According to IIT, a MICS specifies the quality of an experience and integrated information WMax its quantity. From the theory follow several results, including: a system of mechanisms may condense into a major complex and non-overlapping minor complexes; the concepts that specify the quality of an experience are always about the complex itself and relate only indirectly to the external environment; anatomical connectivity influences complexes and associated MICS; a complex can generate a MICS even if its elements are inactive; simple systems can be minimally conscious; complicated systems can be unconscious; there can be true ``zombies'' \textendash{} unconscious feed-forward systems that are functionally equivalent to conscious complexes.},
  langid = {english},
  keywords = {【内容】：整合信息论,【学科】：认知科学,【来源】：集智学园,【进度】：准备进行},
  file = {/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Oizumi_Albantakis_et-al_2014_From the Phenomenology to the Mechanisms of Consciousness - Integrated.pdf;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Oizumi_Albantakis_et-al_2014_From the Phenomenology to the Mechanisms of Consciousness - Integrated(机器翻译).docx;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Oizumi_Albantakis_et-al_2014_From the Phenomenology to the Mechanisms of Consciousness - Integrated(演示文稿).pdf}
}

@misc{Reser_2022_ComputationalArchitectureMachineConsciousnessArtificialSuperintelligenceUpdatingWorkingMemoryIteratively,
  title = {A {{Computational Architecture}} for {{Machine Consciousness}} and {{Artificial Superintelligence}}: {{Updating Working Memory Iteratively}}},
  author = {Reser, Jared Edward},
  year = {2022},
  month = may,
  abstract = {This theoretical article examines how to construct human-like working memory and thought processes within a computer. There should be two working memory stores, one analogous to sustained firing in association cortex, and one analogous to synaptic potentiation in the cerebral cortex. These stores must be constantly updated with new representations that arise from either environmental stimulation or internal processing. They should be updated continuously, and in an iterative fashion, meaning that, in the next state, some items in the set of coactive items should always be retained. Thus, the set of concepts coactive in working memory will evolve gradually and incrementally over time. This makes each state is a revised iteration of the preceding state and causes successive states to overlap and blend with respect to the set of representations they contain. As new representations are added and old ones are subtracted, some remain active for several seconds over the course of these changes. This persistent activity, similar to that used in artificial recurrent neural networks, is used to spread activation energy throughout the global workspace to search for the next associative update. The result is a chain of associatively linked intermediate states that are capable of advancing toward a solution or goal. Iterative updating is conceptualized here as an information processing strategy, a computational and neurophysiological determinant of the stream of thought, and an algorithm for designing and programming artificial intelligence.},
  keywords = {【学科】：AI,【感觉】：有趣,【感觉】：重要,【进度】：准备进行},
  file = {/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Reser_2022_A Computational Architecture for Machine Consciousness and Artificial.docx;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Reser_2022_A Computational Architecture for Machine Consciousness and Artificial.md;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Reser_2022_A Computational Architecture for Machine Consciousness and Artificial.pdf}
}

@article{Signorelli.Wang.ea_2021_CompositionalModelConsciousnessBasedConsciousnessOnly,
  title = {A {{Compositional Model}} of {{Consciousness Based}} on {{Consciousness-Only}}},
  author = {Signorelli, Camilo Miguel and Wang, Quanlong and Khan, Ilyas},
  year = {2021},
  month = mar,
  journal = {Entropy},
  volume = {23},
  number = {3},
  pages = {308},
  issn = {1099-4300},
  doi = {10.3390/e23030308},
  abstract = {Scientific studies of consciousness rely on objects whose existence is assumed to be independent of any consciousness. On the contrary, we assume consciousness to be fundamental, and that one of the main features of consciousness is characterized as being other-dependent. We set up a framework which naturally subsumes this feature by defining a compact closed category where morphisms represent conscious processes. These morphisms are a composition of a set of generators, each being specified by their relations with other generators, and therefore co-dependent. The framework is general enough and fits well into a compositional model of consciousness. Interestingly, we also show how our proposal may become a step towards avoiding the hard problem of consciousness, and thereby address the combination problem of conscious experiences.},
  langid = {english},
  keywords = {【学科】：意识理论,【感觉】：有趣,【方法】：范畴论,【进度】：准备进行},
  file = {/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Signorelli_Wang_et-al_2021_A Compositional Model of Consciousness Based on Consciousness-Only.pdf;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Signorelli_Wang_et-al_2021_A Compositional Model of Consciousness Based on Consciousness-Only(机器翻译).docx}
}

@article{Signorelli.Wang.ea_2021_Reasoningconsciousexperienceaxiomaticgraphicalmathematics,
  title = {Reasoning about Conscious Experience with Axiomatic and Graphical Mathematics},
  author = {Signorelli, Camilo Miguel and Wang, Quanlong and Coecke, Bob},
  year = {2021},
  month = oct,
  journal = {Consciousness and Cognition},
  volume = {95},
  pages = {103168},
  issn = {10538100},
  doi = {10.1016/j.concog.2021.103168},
  abstract = {We cast aspects of consciousness in axiomatic mathematical terms, using the graphical calculus of general process theories (a.k.a symmetric monoidal categories and Frobenius algebras therein). This calculus exploits the ontological neutrality of process theories. A toy example using the axiomatic calculus is given to show the power of this approach, recovering other aspects of conscious experience, such as external and internal subjective distinction, privacy or unread\- ability of personal subjective experience, and phenomenal unity, one of the main issues for sci\- entific studies of consciousness. In fact, these features naturally arise from the compositional nature of axiomatic calculus.},
  langid = {english},
  keywords = {【学科】：意识理论,【感觉】：有趣,【方法】：范畴论,【进度】：准备进行},
  file = {/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Signorelli_Wang_et-al_2021_Reasoning about conscious experience with axiomatic and graphical mathematics.pdf;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Signorelli_Wang_et-al_2021_Reasoning about conscious experience with axiomatic and graphical mathematics(机器翻译).docx}
}

@article{Tononi:2015,
  title = {Integrated Information Theory},
  author = {Tononi, G.},
  year = {2015},
  journal = {Scholarpedia},
  volume = {10},
  number = {1},
  pages = {4164},
  doi = {10/gfwjq5},
  keywords = {【内容】：整合信息论,【学科】：认知科学},
  file = {/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Tononi_2015_Integrated information theory.webarchive}
}

@article{Tononi.Boly.ea_2016_Integratedinformationtheoryconsciousnessitsphysicalsubstrate,
  ids = {Tononi.Boly.ea_2016},
  title = {Integrated Information Theory: From Consciousness to Its Physical Substrate},
  shorttitle = {Integrated Information Theory},
  author = {Tononi, Giulio and Boly, Melanie and Massimini, Marcello and Koch, Christof},
  year = {2016},
  month = jul,
  journal = {Nature Reviews Neuroscience},
  volume = {17},
  number = {7},
  pages = {450--461},
  issn = {1471-003X, 1471-0048},
  doi = {10/f8rbxc},
  abstract = {In this Opinion article, we discuss how integrated information theory accounts for several aspects of the relationship between consciousness and the brain. Integrated information theory starts from the essential properties of phenomenal experience, from which it derives the requirements for the physical substrate of consciousness. It argues that the physical substrate of consciousness must be a maximum of intrinsic cause\textendash effect power and provides a means to determine, in principle, the quality and quantity of experience. The theory leads to some counterintuitive predictions and can be used to develop new tools for assessing consciousness in non-communicative patients.},
  langid = {english},
  keywords = {【内容】：因果涌现,【内容】：整合信息论,【学科】：复杂系统,【学科】：认知科学,【来源】：集智学园,【进度】：准备进行,【项目】：因果涌现读书会},
  file = {/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Tononi_Boly_et-al_2016_Integrated information theory - from consciousness to its physical substrate.pdf;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Tononi_Boly_et-al_2016_Integrated information theory - from consciousness to its physical substrate(机器翻译).docx;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Tononi_Boly_et-al_2016_Integrated information theory - from consciousness to its physical substrate(演示文稿).pdf}
}

@article{williams2010nonnegative,
  title = {Nonnegative Decomposition of Multivariate Information},
  author = {Williams, Paul L. and Beer, Randall D.},
  year = {2010},
  eprint = {1004.2515},
  eprinttype = {arxiv},
  primaryclass = {cs.IT},
  archiveprefix = {arXiv},
  keywords = {【内容】：互信息,【内容】：因果涌现,【内容】：降维,【学科】：信息论,【学科】：复杂系统,【进度】：准备进行,【项目】：因果涌现读书会},
  file = {/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Williams_Beer_2010_Nonnegative decomposition of multivariate information.md;/Users/ethan/LocalFiles/ReferencesFile/Zotero_assets/Williams_Beer_2010_Nonnegative decomposition of multivariate information.pdf}
}


